<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>subsymbol.org (computational linguistics)</title><link>http://www.subsymbol.org/</link><description></description><language>en</language><lastBuildDate>Mon, 02 Mar 2015 15:12:00 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The Grammaticality Continuum</title><link>http://www.subsymbol.org/posts/the-grammaticality-continuum.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;p&gt;Yesterday I was thinking about implementing &lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt; (LDA).
LDA is used for &lt;em&gt;topic modelling&lt;/em&gt; — inducing a set of topics, such that a set of natural language documents can be represented by a mixture of those topics.
This is then used to estimate document similarity, and related information retrieval tasks.&lt;/p&gt;
&lt;p&gt;The first step in such a project is to &lt;em&gt;tokenise&lt;/em&gt; — to break up the text into words, removing attached punctuation, and regularising things like capitalisation.
When looking at the words in a document for the purposes of topic modelling,
it seems appropriate to merge word forms with the same root, or stem, instead of having each form of the "same" word represented individually.
The canonical way to tokenise for topic modelling involves stemming, and it also involves removing &lt;em&gt;stop words&lt;/em&gt; — words like "the", and "and" that are more syntactic than semantic.&lt;/p&gt;
&lt;p&gt;I am not entirely convinced that this latter is appropriate.
The reason is that the grammatically of words exists on a continuum.
Even the word "the" carries semantic weight, though its main function is probably to signal the boundaries of syntactic chunks.&lt;/p&gt;
&lt;hr class="docutils"&gt;
&lt;blockquote&gt;
&lt;p&gt;My favourite example of the syntactic function of "the" comes from &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Petr_Beckmann"&gt;Petr Beckmann&lt;/a&gt; 's book &lt;em&gt;The structure of language: a new approach&lt;/em&gt;, which has profoundly influenced me since my undergraduate days.
In it he shows how the following headline is disambiguated by the placement of "the" before or after "biting":&lt;/p&gt;
&lt;p&gt;"Man Refuses to Give up Biting Dog"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr class="docutils"&gt;
&lt;p&gt;A couple of years ago at &lt;a class="reference external" href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=18614"&gt;the NAACL conference&lt;/a&gt;,
there was a session where a few prominent computational linguists presented their favourite papers from the past.
Eugene Charniak presented Kenneth Church's 2000 COLING paper: &lt;em&gt;Empirical Estimates of Adaptation:&lt;/em&gt;
&lt;em&gt;The chance of Two Noriegas is closer to&lt;/em&gt; &lt;span class="math"&gt;\(p/2\)&lt;/span&gt;
 &lt;em&gt;than&lt;/em&gt; &lt;span class="math"&gt;\(p^2\)&lt;/span&gt;
.
It introduced a measure of adaptation for language models based on how much a recent occurrence of a word increases its tendency to occur beyond what is already expected.&lt;/p&gt;
&lt;p&gt;Charniak used this paper as a background with which to present a new idea about the way the prior likelihood of a word predicts its future occurrences.
He divided words into sets according to how well their priors predicted them.
Stop words were most likely to be true to priors, and content words least, with verbs at the end of the spectrum.&lt;/p&gt;
&lt;p&gt;At the time I took this as evidence for the stratification of grammaticality.
Because of this stratification, treating stop words as a special set appears arbitrary and oversimplified.
I expect that leaving stop words in a topic model would simply result in having some topics that are distributed more evenly throughout the corpus of documents.
These topics would discriminate among documents poorly.
However, this result should also be part of a continuum.
It may be useful to account for the distribution of topics when using them in LDA, in a way analogous to &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;inverse document frequency&lt;/a&gt; in &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_analysis"&gt;latent semantic analysis&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils"&gt;
&lt;p&gt;More generally, I am interested in the phenomenon that words vary in their semantic and syntactic load.
Even just within semantics, a morpheme may carry more than one meaning that cannot be decomposed linguistically.
For example, "uncle" is male, and while we could assign "uncle" a set of semantic features in a computer system in order to reason about its meaning, those features are only implicit in English.
In &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Logogram"&gt;logographic writing systems&lt;/a&gt; this is all the more apparent.&lt;/p&gt;
&lt;p&gt;This simultaneity of features in an apparently linear system is, to me, one of the most interesting aspects of language, and one of the reasons computational linguistics is difficult and rewarding.&lt;/p&gt;</description><category>blog</category><category>computational linguistics</category><category>lda</category><category>lsa</category><category>mathjax</category><guid>http://www.subsymbol.org/posts/the-grammaticality-continuum.html</guid><pubDate>Fri, 25 Jul 2014 15:20:07 GMT</pubDate></item><item><title>Brief notes from the Front Range NLP meeting</title><link>http://www.subsymbol.org/posts/201311brief-notes-from-front-range-nlp-meeting.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;p&gt;Last night I led &lt;a href="http://www.meetup.com/Front-Range-NLP-Natural-Language-Processing/events/146468962/"&gt;the discussion&lt;/a&gt; on Noah Smith's paper &lt;a href="http://arxiv.org/abs/1207.0245"&gt;Adversarial Evaluation for Models of Natural Language&lt;/a&gt;.  It was an interesting and animated discussion, and most of the valuable content was in the talking, but I'm posting my &lt;a href="http://www.cs.toronto.edu/~amber/adversarial_slides.html"&gt;slides&lt;/a&gt; for reference.  I also talked a bit about &lt;a href="https://github.com/lamber/malaprop"&gt;Malaprop&lt;/a&gt;, which builds an adversarial task along with a correction task.&lt;/p&gt;</description><category>blog</category><category>computational linguistics</category><guid>http://www.subsymbol.org/posts/201311brief-notes-from-front-range-nlp-meeting.html</guid><pubDate>Thu, 07 Nov 2013 15:13:00 GMT</pubDate></item><item><title>Malaprop v0.1.0</title><link>http://www.subsymbol.org/posts/201303malaprop-v010.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;blockquote&gt;"...she's as headstrong as an allegory on the banks of Nile." &lt;/blockquote&gt;&lt;p align="right"&gt;— Mrs. Malaprop, in Sheridan's &lt;u&gt;The Rivals&lt;/u&gt;   &lt;/p&gt; &lt;hr&gt;&lt;p&gt;As a contribution to the adversarial evaluation paradigm, I have released my first version of &lt;a href="https://github.com/lamber/malaprop"&gt;Malaprop&lt;/a&gt; &lt;a href="http://www.subsymbol.org/posts/201303malaprop-v010.html"&gt;⁰&lt;/a&gt;, a project involving transformations of natural text that result in some words being replaced by real-word near neighbours.  &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;The Adversarial Evaluation Model for Natural Language Processing&lt;/h3&gt;&lt;p&gt;Noah Smith recently proposed a &lt;a href="http://arxiv.org/abs/1207.0245"&gt;framework for evaluating linguistic models based on adversarial roles&lt;/a&gt; &lt;a href="http://www.subsymbol.org/posts/201303malaprop-v010.html"&gt;¹&lt;/a&gt;. In essence, if you have a sufficiently good linguistic model, you should be able to differentiate between a sample of natural language and an artificially altered sample. An entity that performs this differentiation is called a &lt;i&gt;Claude&lt;/i&gt;. At the same time, having a good linguistic model should also enable you to transform a sample of natural language in a way that preserves its linguistic properties; that is, that makes it hard for a Claude to tell which was the original. An entity that performs this transformation is called a &lt;i&gt;Zellig&lt;/i&gt;. These tasks are complementary. &lt;/p&gt;&lt;p&gt;This framework is reminiscent of the &lt;a href="http://en.wikipedia.org/wiki/Ciphertext_indistinguishability"&gt;cryptographic indistinguishability property&lt;/a&gt;, in which an attacker chooses two plaintexts to give to an Oracle. The Oracle chooses one and encrypts it. The encryption scheme is considered secure if the attacker can not guess at better than chance which of the two plaintexts corresponds to the Oracle's ciphertext. &lt;/p&gt;&lt;p&gt;Even though encryption schemes are constructed mathematically, questions of security are always empirical. The notion of Provable Security is regarded with skepticism (at least by some); schemes are considered tentatively secure based on withstanding attempts to be broken. Similarly, it would take an array of independent Claude's all unable to guess at better than chance to support the claim that a given Zellig had hit upon a truly linguistic-structure-preserving transformation. Likewise, if an array of independent Zelligs can't fool a given Claude, that would support a strong claim about his ability to recognize natural language. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;A Real-Word Error Corpus&lt;/h3&gt;&lt;p&gt;In this spirit, I've just released Malaprop v0.1.0.  It creates a corpus of real-word errors embedded in text. It was designed to work with &lt;a href="http://www.psych.ualberta.ca/~westburylab/downloads/westburylab.wikicorp.download.html"&gt;Westbury Lab's Wikipedia corpora&lt;/a&gt;, but can be used with any text. &lt;/p&gt;&lt;p&gt;The code acts as a noisy channel, randomly inserting &lt;a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"&gt;Damerau-Levenshtein&lt;/a&gt; errors at the character level as a word is passed through. If the resulting string is a &lt;i&gt;real word&lt;/i&gt; — that is, a sufficiently frequent word in the original corpus — the new word replaces the original. &lt;/p&gt;&lt;p&gt;I intend to use this corpus to evaluate algorithms that correct orthographical errors. However, it could be used quite generally as just one Zellig in what I hope becomes a large body of such resources.   &lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Notes &lt;/h3&gt;&lt;p&gt;&lt;a name="%E2%81%B0"&gt;⁰&lt;/a&gt;The term malapropism was first used in the context of the computational linguistics task of real-word error detection and correction by David St-Onge in 1995 in his Master's thesis, &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.596"&gt;Detecting and Correcting Malapropisms with Lexical Chains &lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;a name="%C2%B9"&gt;¹&lt;/a&gt;&lt;a href="http://arxiv.org/abs/1207.0245"&gt;Noah A. Smith. Adversarial Evaluation for Models of Natural Language. CoRR abs/1207.0245 2012 &lt;/a&gt;&lt;/p&gt;</description><category>blog</category><category>computational linguistics</category><category>cryptography</category><category>evaluation</category><category>open access</category><category>open source</category><guid>http://www.subsymbol.org/posts/201303malaprop-v010.html</guid><pubDate>Thu, 07 Mar 2013 22:05:00 GMT</pubDate></item></channel></rss>