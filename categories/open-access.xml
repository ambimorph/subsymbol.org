<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>subsymbol.org (open access)</title><link>http://www.subsymbol.org/</link><description></description><language>en</language><lastBuildDate>Thu, 24 Jul 2014 10:09:21 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Malaprop v0.1.0</title><link>http://www.subsymbol.org/posts/201303malaprop-v010.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;blockquote&gt;"...she's as headstrong as an allegory on the banks of Nile." &lt;/blockquote&gt;&lt;p align="right"&gt;— Mrs. Malaprop, in Sheridan's &lt;u&gt;The Rivals&lt;/u&gt;   &lt;/p&gt; &lt;hr&gt;&lt;p&gt;As a contribution to the adversarial evaluation paradigm, I have released my first version of &lt;a href="https://github.com/lamber/malaprop"&gt;Malaprop&lt;/a&gt; &lt;a href="http://www.subsymbol.org/posts/201303malaprop-v010.html"&gt;⁰&lt;/a&gt;, a project involving transformations of natural text that result in some words being replaced by real-word near neighbours.  &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;The Adversarial Evaluation Model for Natural Language Processing&lt;/h3&gt;&lt;p&gt;Noah Smith recently proposed a &lt;a href="http://arxiv.org/abs/1207.0245"&gt;framework for evaluating linguistic models based on adversarial roles&lt;/a&gt; &lt;a href="http://www.subsymbol.org/posts/201303malaprop-v010.html"&gt;¹&lt;/a&gt;. In essence, if you have a sufficiently good linguistic model, you should be able to differentiate between a sample of natural language and an artificially altered sample. An entity that performs this differentiation is called a &lt;i&gt;Claude&lt;/i&gt;. At the same time, having a good linguistic model should also enable you to transform a sample of natural language in a way that preserves its linguistic properties; that is, that makes it hard for a Claude to tell which was the original. An entity that performs this transformation is called a &lt;i&gt;Zellig&lt;/i&gt;. These tasks are complementary. &lt;/p&gt;&lt;p&gt;This framework is reminiscent of the &lt;a href="http://en.wikipedia.org/wiki/Ciphertext_indistinguishability"&gt;cryptographic indistinguishability property&lt;/a&gt;, in which an attacker chooses two plaintexts to give to an Oracle. The Oracle chooses one and encrypts it. The encryption scheme is considered secure if the attacker can not guess at better than chance which of the two plaintexts corresponds to the Oracle's ciphertext. &lt;/p&gt;&lt;p&gt;Even though encryption schemes are constructed mathematically, questions of security are always empirical. The notion of Provable Security is regarded with skepticism (at least by some); schemes are considered tentatively secure based on withstanding attempts to be broken. Similarly, it would take an array of independent Claude's all unable to guess at better than chance to support the claim that a given Zellig had hit upon a truly linguistic-structure-preserving transformation. Likewise, if an array of independent Zelligs can't fool a given Claude, that would support a strong claim about his ability to recognize natural language. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;A Real-Word Error Corpus&lt;/h3&gt;&lt;p&gt;In this spirit, I've just released Malaprop v0.1.0.  It creates a corpus of real-word errors embedded in text. It was designed to work with &lt;a href="http://www.psych.ualberta.ca/~westburylab/downloads/westburylab.wikicorp.download.html"&gt;Westbury Lab's Wikipedia corpora&lt;/a&gt;, but can be used with any text. &lt;/p&gt;&lt;p&gt;The code acts as a noisy channel, randomly inserting &lt;a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"&gt;Damerau-Levenshtein&lt;/a&gt; errors at the character level as a word is passed through. If the resulting string is a &lt;i&gt;real word&lt;/i&gt; — that is, a sufficiently frequent word in the original corpus — the new word replaces the original. &lt;/p&gt;&lt;p&gt;I intend to use this corpus to evaluate algorithms that correct orthographical errors. However, it could be used quite generally as just one Zellig in what I hope becomes a large body of such resources.   &lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Notes &lt;/h3&gt;&lt;p&gt;&lt;a name="%E2%81%B0"&gt;⁰&lt;/a&gt;The term malapropism was first used in the context of the computational linguistics task of real-word error detection and correction by David St-Onge in 1995 in his Master's thesis, &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.596"&gt;Detecting and Correcting Malapropisms with Lexical Chains &lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;a name="%C2%B9"&gt;¹&lt;/a&gt;&lt;a href="http://arxiv.org/abs/1207.0245"&gt;Noah A. Smith. Adversarial Evaluation for Models of Natural Language. CoRR abs/1207.0245 2012 &lt;/a&gt;&lt;/p&gt;</description><category>computational linguistics</category><category>cryptography</category><category>evaluation</category><category>open access</category><category>open source</category><guid>http://www.subsymbol.org/posts/201303malaprop-v010.html</guid><pubDate>Thu, 07 Mar 2013 22:05:00 GMT</pubDate></item><item><title>Publishing a Paper without the Code is Not Enough</title><link>http://www.subsymbol.org/posts/201301publishing-paper-without-code-is-not.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;h3&gt;Shoulders to stand on&lt;/h3&gt;&lt;p&gt;Here are a couple of anecdotes demonstrating that without access to the implementation of an experiment, scientific progress is halted. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;When I first began my master's degree, I made serious starts on a few ideas that eventually were blocked for lack of access of one kind or another. In one case I was unable to make progress because my idea involved building on the work of another who would not release his code to me. &lt;p&gt;(To give the benefit of the doubt, I now assume that the code was sloppy, missing pieces, or poorly documented, and he was simply embarrassed. Or perhaps there was some other valid reason. I don't know; he never gave one.) &lt;/p&gt;&lt;p&gt;Regardless of his motivation, this was a suboptimal outcome for him as well as for me, because his work could have been extended and improved at that time, but it was not. I had already spent a lot of time working out design details of my project, under the assumption that the code would be available. Nonetheless, I felt it would be too much work to replicate his entire thesis, and so I moved on to another topic. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;A colleague in my cohort had a related experience in which she did replicate the work of a renowned scientist in the field, in order to attempt some improvements. In her case, the code she wrote, guided by interpreting the relevant paper, didn't do what the paper claimed. It is not clear whether this was because she was missing some vital methodology, or whether the claims were not justified. Neither is an acceptable result. &lt;/li&gt;&lt;/ul&gt;Having talked with other students, I know that these stories are not of isolated experiences. &lt;p&gt;Surely, you too have recently come across some fascinating scientific results that gave you an idea you wanted to implement right away, but to your dismay, you quickly realized that you would have to start from scratch. The algorithm was complex. Maybe the data was not available.   With disappointment, you realized that it would take you weeks or months of error-prone coding just to get to the baseline. Then you would not be confident that you implemented it in exactly the same way as the original, and there would be no direct comparison. So you wrote it down in your Someday/Maybe file and forgot about it. This happens to me constantly, and it just seems tragic and unnecessary.  &lt;/p&gt;&lt;h3&gt;My own dog food&lt;/h3&gt;&lt;p&gt;Later, I too did a replication study. In this case my advisor and a fellow student wanted to compare their work to known work on the same problem. However, neither the code nor the data was released (the data was proprietary), and the evaluations were not published in a form comparable to more modern evaluation. Luckily for us, the method worked beautifully, and now everyone can see that more clearly. &lt;/p&gt;&lt;p&gt;In an ironic turn, being a novice programmer at the time, my replication code was disorganized. Some of it was lost. It was not under version control until quite late and had few tests of correctness. I now had grounds to empathize with the colleague I earlier felt slighted by. However, I am in the fortunate position of having had to take a forced break before graduating, during which I learned basic software engineering skills, and had ample time to think about this issue. &lt;/p&gt;&lt;p&gt;I am now re-rewriting the entire code base such that the experiments can be completely replicated from scratch. Every step will be included in the code I release, including "trivial" command line glue. Although every module has tests, no code is invulnerable. Bugs may well turn up, and if they do, they will be there for analysis and repair by anyone who cares. Most importantly, if anyone wants to know exactly what I did, they will not have to scour the paper for hints. Similarly, all the data will be at their disposal for analysis, if mine lacks the answer to any particular unforeseeable analytical question.  &lt;/p&gt;&lt;h3&gt;This should be standard&lt;/h3&gt;&lt;p&gt;In 2013, there is no excuse for publishing a paper in applied computer science without releasing the code; a paper by itself is an unsubstantiated claim. Unlike in biology, or other physical fields, applied computer science results should be trivial to replicate by anyone with the right equipment. Moreover, not releasing the code gives your lab an unsportsmanlike advantage: you get to claim the results, perhaps state-of-the-art results, and you get to stay on the cutting edge, because no one else has time to catch up. &lt;/p&gt;&lt;p&gt;Many universities and labs now have projects under open licenses, but it is by no means a standard, and it is not a prerequisite for publication. We ought to change this.&lt;/p&gt;</description><category>open access</category><category>open source</category><category>science</category><guid>http://www.subsymbol.org/posts/201301publishing-paper-without-code-is-not.html</guid><pubDate>Fri, 18 Jan 2013 23:11:00 GMT</pubDate></item></channel></rss>