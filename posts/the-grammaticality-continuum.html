<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The Grammaticality Continuum | subsymbol.org</title>
<link href="../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="http://www.subsymbol.org/posts/the-grammaticality-continuum.html">
<!--[if lt IE 9]><script src="/assets/js/html5.js"></script><![endif]--><meta name="author" content="L. Amber Wilcox-O'Hearn">
<meta name="og:title" content="The Grammaticality Continuum">
<meta name="og:url" content="http://www.subsymbol.org/posts/the-grammaticality-continuum.html">
<meta name="og:description" content="Yesterday I was thinking about implementing Latent Dirichlet Allocation (LDA).
LDA is used for topic modelling — inducing a set of topics, such that a set of natural language documents can be represen">
<meta name="og:site_name" content="subsymbol.org">
<meta name="og:type" content="article">
</head>
<body>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://www.subsymbol.org/">

                <span id="blog-title">subsymbol.org</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
<li>
<a href="../archive.html">Archive</a>
                </li>
<li>
<a href="../categories/">Tags</a>
                </li>
<li>
<a href="../blog/">Blog</a>
                </li>
<li>
<a href="../journal/">Journal</a>
                </li>
<li>
<a href="../rss.xml">RSS feed</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="the-grammaticality-continuum.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">The Grammaticality Continuum</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">L. Amber Wilcox-O'Hearn</span></p>
            <p class="dateline"><a href="#" rel="bookmark"><time class="published dt-published" datetime="2014-07-25T15:20:07+00:00" itemprop="datePublished" title="Publication date">2014-07-25 15:20</time></a></p>
                <p class="commentline">
        
    <a href="the-grammaticality-continuum.html#disqus_thread" data-disqus-identifier="cache/posts/the-grammaticality-continuum.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="the-grammaticality-continuum.rst" id="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>Yesterday I was thinking about implementing <em>Latent Dirichlet Allocation</em> (LDA).
LDA is used for <em>topic modelling</em> — inducing a set of topics, such that a set of natural language documents can be represented by a mixture of those topics.
This is then used to estimate document similarity, and related information retrieval tasks.</p>
<p>The first step in such a project is to <em>tokenise</em> — to break up the text into words, removing attached punctuation, and regularising things like capitalisation.
When looking at the words in a document for the purposes of topic modelling,
it seems appropriate to merge word forms with the same root, or stem, instead of having each form of the "same" word represented individually.
The canonical way to tokenise for topic modelling involves stemming, and it also involves removing <em>stop words</em> — words like "the", and "and" that are more syntactic than semantic.</p>
<p>I am not entirely convinced that this latter is appropriate.
The reason is that the grammatically of words exists on a continuum.
Even the word "the" carries semantic weight, though its main function is probably to signal the boundaries of syntactic chunks.</p>
<hr class="docutils">
<blockquote>
<p>My favourite example of the syntactic function of "the" comes from <a class="reference external" href="http://en.wikipedia.org/wiki/Petr_Beckmann">Petr Beckmann</a> 's book <em>The structure of language: a new approach</em>, which has profoundly influenced me since my undergraduate days.
In it he shows how the following headline is disambiguated by the placement of "the" before or after "biting":</p>
<p>"Man Refuses to Give up Biting Dog"</p>
</blockquote>
<hr class="docutils">
<p>A couple of years ago at <a class="reference external" href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=18614">the NAACL conference</a>,
there was a session where a few prominent computational linguists presented their favourite papers from the past.
Eugene Charniak presented Kenneth Church's 2000 COLING paper: <em>Empirical Estimates of Adaptation:</em>
<em>The chance of Two Noriegas is closer to</em> <span class="math">\(p/2\)</span>
 <em>than</em> <span class="math">\(p^2\)</span>
.
It introduced a measure of adaptation for language models based on how much a recent occurrence of a word increases its tendency to occur beyond what is already expected.</p>
<p>Charniak used this paper as a background with which to present a new idea about the way the prior likelihood of a word predicts its future occurrences.
He divided words into sets according to how well their priors predicted them.
Stop words were most likely to be true to priors, and content words least, with verbs at the end of the spectrum.</p>
<p>At the time I took this as evidence for the stratification of grammaticality.
Because of this stratification, treating stop words as a special set appears arbitrary and oversimplified.
I expect that leaving stop words in a topic model would simply result in having some topics that are distributed more evenly throughout the corpus of documents.
These topics would discriminate among documents poorly.
However, this result should also be part of a continuum.
It may be useful to account for the distribution of topics when using them in LDA, in a way analogous to <a class="reference external" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">inverse document frequency</a> in <a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">latent semantic analysis</a>.</p>
<hr class="docutils">
<p>More generally, I am interested in the phenomenon that words vary in their semantic and syntactic load.
Even just within semantics, a morpheme may carry more than one meaning that cannot be decomposed linguistically.
For example, "uncle" is male, and while we could assign "uncle" a set of semantic features in a computer system in order to reason about its meaning, those features are only implicit in English.
In <a class="reference external" href="http://en.wikipedia.org/wiki/Logogram">logographic writing systems</a> this is all the more apparent.</p>
<p>This simultaneity of features in an apparently linear system is, to me, one of the most interesting aspects of language, and one of the reasons computational linguistics is difficult and rewarding.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../categories/computational-linguistics.html" rel="tag">computational linguistics</a></li>
           <li><a class="tag p-category" href="../categories/lda.html" rel="tag">lda</a></li>
           <li><a class="tag p-category" href="../categories/lsa.html" rel="tag">lsa</a></li>
           <li><a class="tag p-category" href="../categories/mathjax.html" rel="tag">mathjax</a></li>
        </ul>
<ul class="pager">
<li class="previous">
                <a href="productively-lost.html" rel="prev" title="Productively Lost">Previous post</a>
            </li>
            <li class="next">
                <a href="polyas-urn.html" rel="next" title="Pólya's Urn">Next post</a>
            </li>
        </ul></nav></aside><section class="comments"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="subsymbolorg",
            disqus_url="http://www.subsymbol.org/posts/the-grammaticality-continuum.html",
        disqus_title="The Grammaticality Continuum",
        disqus_identifier="cache/posts/the-grammaticality-continuum.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="//disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});</script><script src="../assets/js/mathjax.js"></script></article><script>var disqus_shortname="subsymbolorg";(function(){var a=document.createElement("script");a.async=true;a.src="//"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->

        <footer><br><br>Contents © 2014         <a href="mailto:amber@cs.toronto.edu">L. Amber Wilcox-O'Hearn</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../assets/js/jquery.min.js"></script><script src="../assets/js/bootstrap.min.js"></script><script src="../assets/js/jquery.colorbox-min.js"></script><!-- Social buttons --><div id="addthisbox" class="addthis_toolbox addthis_peekaboo_style addthis_default_style addthis_label_style addthis_32x32_style">
<a class="addthis_button_more">Share</a>
<ul>
<li>
<a class="addthis_button_facebook"></a>
</li>
<li>
<a class="addthis_button_google_plusone_share"></a>
</li>
<li>
<a class="addthis_button_linkedin"></a>
</li>
<li>
<a class="addthis_button_twitter"></a>
</li>
</ul>
</div>
<script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4f7088a56bb93798"></script><!-- End of social buttons --><script>jQuery("a.image-reference").colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script>
</body>
</html>
