<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Notes on Fragment Grammars | subsymbol.org</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="http://www.subsymbol.org/posts/notes-on-fragment-grammars.html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><meta name="author" content="L. Amber Wilcox-O'Hearn">
<link rel="prev" href="two-inotify-pitfalls.html" title="Two inotify Pitfalls" type="text/html">
<link rel="next" href="i-did-it.html" title="I did it!" type="text/html">
<meta property="og:site_name" content="subsymbol.org">
<meta property="og:title" content="Notes on Fragment Grammars">
<meta property="og:url" content="http://www.subsymbol.org/posts/notes-on-fragment-grammars.html">
<meta property="og:description" content="Last week I read
Fragment Grammars: Exploring Computation and Reuse in Language
by Timothy J. O'Donnell, Joshua B. Tenenbaum, and Noah D. Goodman.
As I mentioned in my journal,
the authors of this tec">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2015-03-02T22:00:26Z">
<meta property="article:tag" content="adaptor grammars">
<meta property="article:tag" content="blog">
<meta property="article:tag" content="fragment grammars">
<meta property="article:tag" content="grammar induction">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="PCFG">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../">

            <span id="blog-title">subsymbol.org</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="notes-on-fragment-grammars.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../categories/blog/" class="nav-link">Blog</a>
                </li>
<li class="nav-item">
<a href="../journal/" class="nav-link">Journal</a>
                </li>
<li class="nav-item">
<a href="../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Notes on Fragment Grammars</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    L. Amber Wilcox-O'Hearn
            </span></p>
            <p class="dateline">
            <a href="#" rel="bookmark">
            <time class="published dt-published" datetime="2015-03-02T22:00:26Z" itemprop="datePublished" title="2015-03-02 22:00">2015-03-02 22:00</time></a>
            </p>
                <p class="commentline">
    
    <a href="notes-on-fragment-grammars.html#disqus_thread" data-disqus-identifier="cache/posts/notes-on-fragment-grammars.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="notes-on-fragment-grammars.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>Last week I read
<a class="reference external" href="http://dspace.mit.edu/handle/1721.1/44963">Fragment Grammars: Exploring Computation and Reuse in Language</a>
by Timothy J. O'Donnell, Joshua B. Tenenbaum, and Noah D. Goodman.</p>
<p>As I mentioned in <a class="reference external" href="http://subsymbol.org/journal/wednesday-february-25th-2015.html">my journal</a>,
the authors of this tech report promise to generalise <a class="reference external" href="http://papers.nips.cc/paper/3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.pdf">adaptor grammars</a> (a variety of <a class="reference external" href="http://en.wikipedia.org/wiki/Stochastic_context-free_grammar">PCFG</a> that uses a <a class="reference external" href="http://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process">Pitman-Yor process</a> to adapt its probabilities based on context) by using a <em>heterogenous</em> lexicon (one that is not married to some prescriptivist notion of linguistic category such as word or morpheme, and thus can include items smaller or larger than words).
The "lexicon" is chosen to optimise between storing a relatively small number of tiny units which require a lot of computation to structure together vs. storing a large number of long strings which cover large stretches of text, but aren't very flexible or general.
In other words, it's a tradeoff in compression vs. computation.</p>
<p>Here are my impressions on first reading.</p>
<p><strong>What I really love about this tech report is that it unpacks a lot of typically presumed knowledge right inside it.</strong></p>
<p>So if you didn't know about PCFGs or memoization, or Chinese restaurant processes, or non-parametric Bayesian methods before, you can get a lot of what you need to know right there.
Of course, The reason a typical conference or journal paper doesn't include such thorough background, is simply that there isn't the space for it.
Moreover, one can usually assume that the audience has the appropriate background, or knows how to acquire it.
Nonetheless, I find it a great pleasure to read something that assumes an educated audience that isn't intimidated by statistical models or equations, but might not know every niche term involved in such a specialised task.</p>
<p><strong>Here are some ways in which reading this paper helped me to grok non-parametric Bayesian techniques.</strong></p>
<p>I had never thought of LDA and related algorithms as <em>stochastic memoisation</em>, which is how they are described here.</p>
<!--  -->
<blockquote>
<p>"A stochastic memoizer wraps a stochastic procedure [i.e a sampler]
in another distribution, called the memoization distribution, which
tells us when to reuse one of the previously computed values, and
when to compute a fresh value from the underlying procedure. To
accomplish this, we generalize the notion of a memotable so that it
stores a distribution for each procedure–plus–arguments
combination."</p>
</blockquote>
<p>I like this description because it is immediately understandable to someone who has used dynamic programming.
We know the value of limiting recomputation (and, again, if you don't, the classic Fibonacci example is right in the paper!),
and now we see this generalised to probabilistically either using the cached value or resampling.
As the authors explain:</p>
<!--  -->
<blockquote>
<p>"If we wrap such a random procedure in a deterministic memoizer, then
it will sample a value the first time it is applied to some arguments,
but forever after, it will return the same value by virtue of
memoization. It is natural to consider making the notion of
memoization itself stochastic, so that sometimes the memoizer returns
a value computed earlier, and sometimes it computes a fresh value."</p>
</blockquote>
<p>I have seen several different presentations of LDA, and not once was it described in this intuitive way.</p>
<p>Further, we can see how using the Chinese Restaurant Process, which is biased to sample what has been sampled before as a "simplicity bias":</p>
<!--  -->
<blockquote>
<p>"all else being equal, when we use the CRP as a stochastic memoizer
we favor reuse of previously computed values."</p>
</blockquote>
<p>An assumption that Gibbs sampling relies on was made clear to me in the explanation of <em>exchangeability</em>.</p>
<!--  -->
<blockquote>
<p>"Intuitively, exchangeability says that the order in which we observed
some data will not make a difference to our inferences about
it. Exchangeability is an important property in Bayesian statistics,
and our inference algorithms below will rely on it crucially. It is
also a desirable property in cognitive models."</p>
<p>"Pitman-Yor processes, multinomial-Dirichlet distributions, and
beta-Binomial distributions are all exchangeable, which means that
we are free to treat any expression e(i) ∈ E as if it were the last
expression sampled during the creation of E.  Our sampling
algorithm leverages this fact by (re-)sampling each p(i) ∈ P
for each expression in turn."</p>
</blockquote>
<p>Even though I knew that the exchangeability was necessary for taking products,
that is, that permutations don't effect the joint distributions,
I hadn't thought about the way this frees us in our sampling order.
If we wanted to add some kind of recency effects to our models, order would, of course, become important.</p>
<p><strong>The real meat of the paper, though, is in describing</strong> <em>Fragment Grammars</em> <strong>as contrasted with</strong> <em>Adaptor Grammars</em>.</p>
<p>This will likely be the topic of the next post.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../categories/adaptor-grammars.html" rel="tag">adaptor grammars</a></li>
            <li><a class="tag p-category" href="../categories/fragment-grammars.html" rel="tag">fragment grammars</a></li>
            <li><a class="tag p-category" href="../categories/grammar-induction.html" rel="tag">grammar induction</a></li>
            <li><a class="tag p-category" href="../categories/nlp.html" rel="tag">NLP</a></li>
            <li><a class="tag p-category" href="../categories/pcfg.html" rel="tag">PCFG</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="two-inotify-pitfalls.html" rel="prev" title="Two inotify Pitfalls">Previous post</a>
            </li>
            <li class="next">
                <a href="i-did-it.html" rel="next" title="I did it!">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
    
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="subsymbol.org",
            disqus_url="http://www.subsymbol.org/posts/notes-on-fragment-grammars.html",
        disqus_title="Notes on Fragment Grammars",
        disqus_identifier="cache/posts/notes-on-fragment-grammars.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="subsymbol.org";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer">
            Contents © 2022         <a href="mailto:amber@cs.toronto.edu">L. Amber Wilcox-O'Hearn</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
