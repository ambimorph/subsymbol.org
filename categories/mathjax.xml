<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>subsymbol.org (mathjax)</title><link>http://www.subsymbol.org/</link><description></description><language>en</language><lastBuildDate>Tue, 29 Jul 2014 17:01:51 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Pólya's Urn</title><link>http://www.subsymbol.org/posts/polyas-urn.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;div class="section" id="balls-in-urns"&gt;
&lt;h2&gt;Balls in Urns&lt;/h2&gt;
&lt;p&gt;If you have studied probability, you are probably familiar with the canonical balls-in-an-urn allegory for understanding discrete probability distributions.
For example, you could imagine an urn containing 1 red ball and 3 green balls.
Drawing a ball from the urn at random represents sampling from a probability distribution where the probability of one outcome is &lt;span class="math"&gt;\(25\%\)&lt;/span&gt;
 and the probability of the other outcome is &lt;span class="math"&gt;\(75\%\)&lt;/span&gt;

We can extend this idea in a variety of ways.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="polya-s-urn"&gt;
&lt;h2&gt;Pólya's Urn&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;Pólya's Urn&lt;/em&gt;, the extension is that whenever you draw a ball from the urn, you not only replace it, but you add an extra ball of the same colour.
So if you happened to draw a green ball in the example above, then the ratio would change from &lt;span class="math"&gt;\(1:3\)&lt;/span&gt;
 to &lt;span class="math"&gt;\(1:4\)&lt;/span&gt;
.
That means on the next draw, you would now have only a &lt;span class="math"&gt;\(20\%\)&lt;/span&gt;
 chance of drawing red.
On the other hand, if you happened to have drawn red, then the ratio would change to &lt;span class="math"&gt;\(2:3\)&lt;/span&gt;
, giving red a probability of &lt;span class="math"&gt;\(40\%\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;This process is interesting, because it has the property that the more often you observe something, the more likely you are to observe it again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="different-starting-conditions"&gt;
&lt;h2&gt;Different starting conditions&lt;/h2&gt;
&lt;p&gt;The way the distribution changes over time depends on the starting conditions.&lt;/p&gt;
&lt;div class="section" id="one-of-each"&gt;
&lt;h3&gt;One of each&lt;/h3&gt;
&lt;p&gt;Let's imagine the simplest case, in which we start with one each of two colours, red and green.
The following table shows the probabilities of getting red on the first three draws, and how each draw changes the probability of the next by changing the proportion of colours in the urn.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;&lt;colgroup&gt;&lt;col width="33%"&gt;&lt;col width="33%"&gt;&lt;col width="33%"&gt;&lt;/colgroup&gt;&lt;thead valign="bottom"&gt;&lt;tr&gt;&lt;th class="head"&gt; &lt;/th&gt;
&lt;th class="head"&gt;RRR&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Draw&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;p(Draw)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;new R:G&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;
&lt;/td&gt;
&lt;td&gt;2:1&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(2/3\)&lt;/span&gt;
&lt;/td&gt;
&lt;td&gt;3:1&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(3/4\)&lt;/span&gt;
&lt;/td&gt;
&lt;td&gt;4:1&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;hr class="docutils"&gt;&lt;p&gt;There are more ways to have drawn two of one colour, and one of the other, than 3 of one colour.
However, because of the way drawing a particular colour reinforces itself,
there is a &lt;span class="math"&gt;\(50\%\)&lt;/span&gt;
 chance of drawing the same colour every time over the first three draws.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;&lt;colgroup&gt;&lt;col width="50%"&gt;&lt;col width="50%"&gt;&lt;/colgroup&gt;&lt;thead valign="bottom"&gt;&lt;tr&gt;&lt;th class="head"&gt;First three draws&lt;/th&gt;
&lt;th class="head"&gt;probability&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td&gt;p(RRR)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 3/4 = 1/4\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(RRG)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/4 = 1/12\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(RGR)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/2 = 1/6\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(RGG)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/2 = 1/6\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(GRR)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/2 = 1/6\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(GRG)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/2 = 1/6\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(GGR)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 1/4 = 1/12\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;p(GGG)&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1/2 \times 2/3 \times 3/4 = 1/4\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class="section" id="ten-of-each"&gt;
&lt;h3&gt;Ten of each&lt;/h3&gt;
&lt;p&gt;Now suppose that we start with 10 each of red and green balls.
In this case, simply drawing a red ball the first time does not change the probability that it will be drawn again nearly as significantly as with the &lt;span class="math"&gt;\(1:1\)&lt;/span&gt;
 starting conditions.
The probability of drawing 3 of the same colour in a row falls to &lt;span class="math"&gt;\(2 \times 10/20 \times 11/21 \times 12/22 = 2/7 \cong  29\%\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;We can view the starting conditions as a list of numbers, one for each starting colour, and call it &lt;em&gt;alpha&lt;/em&gt; (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;
).
So our first example had &lt;span class="math"&gt;\(\alpha = [1, 3]\)&lt;/span&gt;
,
our second example had &lt;span class="math"&gt;\(\alpha = [1, 1]\)&lt;/span&gt;
,
and our third example had &lt;span class="math"&gt;\(\alpha = [10, 10]\)&lt;/span&gt;
,&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="higher-returns"&gt;
&lt;h3&gt;Higher returns&lt;/h3&gt;
&lt;p&gt;On the other hand, imagine if we started with 1 each of red and green, but instead of increasing the number of balls by 1 when we draw a colour, we increased it by 10.
Now every draw has a much stronger effect.
The probability of drawing the same colour 3 times in a row would now be &lt;span class="math"&gt;\(2 \times 1/2 \times 11/12 \times 21/22 = 7/8 \cong 88\%\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;We could even have a particular increase number for each colour, and have another list, called &lt;em&gt;beta&lt;/em&gt; (&lt;span class="math"&gt;\(\beta\)&lt;/span&gt;
).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="more-colours"&gt;
&lt;h3&gt;More colours&lt;/h3&gt;
&lt;p&gt;Another way to change the starting conditions is to increase the number of colours.
If our starting urn had one each of 10 different colours, then, again, when we draw the first ball, it has much less of an effect on the chance of drawing it again.
We can call the number of colours &lt;span class="math"&gt;\(n\)&lt;/span&gt;
.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="try-it"&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;p&gt;Use the sliders to choose &lt;span class="math"&gt;\(n\)&lt;/span&gt;
 colours, and a single &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;
 and &lt;span class="math"&gt;\(beta\)&lt;/span&gt;
 for all colours. Try drawing balls from the urn, and see how the urn changes.  At any time you can display the urn in rank order or reset to the current slider position.&lt;/p&gt;
n: &lt;b id="n-output"&gt;&lt;/b&gt;
&lt;input class="n-slider" type="range" min="1" max="8"&gt;
α: &lt;b id="alpha-output"&gt;&lt;/b&gt;
&lt;input class="alpha-slider" type="range" min="1" max="8"&gt;
β: &lt;b id="beta-output"&gt;&lt;/b&gt;
&lt;input class="beta-slider" type="range" min="1" max="8"&gt;&lt;br&gt;&lt;p id="urn"&gt;Urn&lt;/p&gt;
&lt;br&gt;&lt;button id="draw"&gt;Draw!&lt;/button&gt;
&lt;button id="reset"&gt;Reset.&lt;/button&gt;
&lt;button id="rank"&gt;Order by rank.&lt;/button&gt;

&lt;script src="http://www.subsymbol.org/scripts/polya.js"&gt;
&lt;/script&gt;&lt;/div&gt;</description><category>mathjax</category><category>probability</category><guid>http://www.subsymbol.org/posts/polyas-urn.html</guid><pubDate>Fri, 25 Jul 2014 17:24:37 GMT</pubDate></item><item><title>The Grammaticality Continuum</title><link>http://www.subsymbol.org/posts/the-grammaticality-continuum.html</link><dc:creator>L. Amber Wilcox-O'Hearn</dc:creator><description>&lt;p&gt;Yesterday I was thinking about implementing &lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt; (LDA).
LDA is used for &lt;em&gt;topic modelling&lt;/em&gt; — inducing a set of topics, such that a set of natural language documents can be represented by a mixture of those topics.
This is then used to estimate document similarity, and related information retrieval tasks.&lt;/p&gt;
&lt;p&gt;The first step in such a project is to &lt;em&gt;tokenise&lt;/em&gt; — to break up the text into words, removing attached punctuation, and regularising things like capitalisation.
When looking at the words in a document for the purposes of topic modelling,
it seems appropriate to merge word forms with the same root, or stem, instead of having each form of the "same" word represented individually.
The canonical way to tokenise for topic modelling involves stemming, and it also involves removing &lt;em&gt;stop words&lt;/em&gt; — words like "the", and "and" that are more syntactic than semantic.&lt;/p&gt;
&lt;p&gt;I am not entirely convinced that this latter is appropriate.
The reason is that the grammatically of words exists on a continuum.
Even the word "the" carries semantic weight, though its main function is probably to signal the boundaries of syntactic chunks.&lt;/p&gt;
&lt;hr class="docutils"&gt;&lt;blockquote&gt;
&lt;p&gt;My favourite example of the syntactic function of "the" comes from &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Petr_Beckmann"&gt;Petr Beckmann&lt;/a&gt; 's book &lt;em&gt;The structure of language: a new approach&lt;/em&gt;, which has profoundly influenced me since my undergraduate days.
In it he shows how the following headline is disambiguated by the placement of "the" before or after "biting":&lt;/p&gt;
&lt;p&gt;"Man Refuses to Give up Biting Dog"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr class="docutils"&gt;&lt;p&gt;A couple of years ago at &lt;a class="reference external" href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=18614"&gt;the NAACL conference&lt;/a&gt;,
there was a session where a few prominent computational linguists presented their favourite papers from the past.
Eugene Charniak presented Kenneth Church's 2000 COLING paper: &lt;em&gt;Empirical Estimates of Adaptation:&lt;/em&gt;
&lt;em&gt;The chance of Two Noriegas is closer to&lt;/em&gt; &lt;span class="math"&gt;\(p/2\)&lt;/span&gt;
 &lt;em&gt;than&lt;/em&gt; &lt;span class="math"&gt;\(p^2\)&lt;/span&gt;
.
It introduced a measure of adaptation for language models based on how much a recent occurrence of a word increases its tendency to occur beyond what is already expected.&lt;/p&gt;
&lt;p&gt;Charniak used this paper as a background with which to present a new idea about the way the prior likelihood of a word predicts its future occurrences.
He divided words into sets according to how well their priors predicted them.
Stop words were most likely to be true to priors, and content words least, with verbs at the end of the spectrum.&lt;/p&gt;
&lt;p&gt;At the time I took this as evidence for the stratification of grammaticality.
Because of this stratification, treating stop words as a special set appears arbitrary and oversimplified.
I expect that leaving stop words in a topic model would simply result in having some topics that are distributed more evenly throughout the corpus of documents.
These topics would discriminate among documents poorly.
However, this result should also be part of a continuum.
It may be useful to account for the distribution of topics when using them in LDA, in a way analogous to &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;inverse document frequency&lt;/a&gt; in &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_analysis"&gt;latent semantic analysis&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils"&gt;&lt;p&gt;More generally, I am interested in the phenomenon that words vary in their semantic and syntactic load.
Even just within semantics, a morpheme may carry more than one meaning that cannot be decomposed linguistically.
For example, "uncle" is male, and while we could assign "uncle" a set of semantic features in a computer system in order to reason about its meaning, those features are only implicit in English.
In &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Logogram"&gt;logographic writing systems&lt;/a&gt; this is all the more apparent.&lt;/p&gt;
&lt;p&gt;This simultaneity of features in an apparently linear system is, to me, one of the most interesting aspects of language, and one of the reasons computational linguistics is difficult and rewarding.&lt;/p&gt;</description><category>computational linguistics</category><category>lda</category><category>lsa</category><category>mathjax</category><guid>http://www.subsymbol.org/posts/the-grammaticality-continuum.html</guid><pubDate>Fri, 25 Jul 2014 15:20:07 GMT</pubDate></item></channel></rss>